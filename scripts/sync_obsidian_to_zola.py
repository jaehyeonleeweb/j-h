#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os, re, shutil, pathlib, sys

# === ê²½ë¡œ ì„¤ì • (í•„ìš” ì‹œ ìˆ˜ì •) ===
VAULT = "/Users/jaehyeonlee/Library/Mobile Documents/iCloud~md~obsidian/Documents/j-h-web"
DEST  = "/Users/jaehyeonlee/web/j-h"

# Vaultì—ì„œ ê°€ì ¸ì˜¬ ì„¹ì…˜ ë£¨íŠ¸ (ë³´ê´€í•¨ ìµœìƒìœ„ í´ë”ë“¤)
SRC_CONTENT_ROOTS = ["about", "contact", "glossary", "method", "shop", "thought", "works"]
ROOT_SECTIONS = set(s.lower() for s in SRC_CONTENT_ROOTS)

# ëª©ì ì§€ì— í•­ìƒ ìˆì–´ì•¼ í•˜ëŠ” ì„¹ì…˜(ì—†ìœ¼ë©´ ìë™ ìƒì„±)
SECTIONS = [
    "",  # í™ˆ
    "about", "contact", "glossary", "method", "shop", "thought", "works",
    "works/project", "works/workshop", "works/workshop-practice",
]

# ğŸ”¤ ì˜ë¬¸/í•œêµ­ì–´ ì„¹ì…˜ íƒ€ì´í‹€
TITLES_EN = {
    "": "all archive", "about": "about", "contact": "contact",
    "glossary": "glossary", "method": "methodology", "shop": "shop",
    "thought": "thought", "works": "works",
    "works/project": "project", "works/workshop": "workshop",
    "works/workshop-practice": "workshop practice",
}
TITLES_KR = {
    "": "ì•„ì¹´ì´ë¸Œ", "about": "ì†Œê°œ", "contact": "ì—°ë½ì²˜",
    "glossary": "ì°¸ì¡°", "method": "ë°©ë²•ë¡ ", "shop": "êµ¬ë§¤",
    "thought": "ìƒê°", "works": "ì‘ì—…",
    "works/project": "í”„ë¡œì íŠ¸", "works/workshop": "ì›Œí¬ìƒµ",
    "works/workshop-practice": "ì›Œí¬ìƒµ ì‹¤ì²œ",
}

# ì„¹ì…˜ë³„ í…œí”Œë¦¿ ë§¤í•‘
SECTION_TEMPLATES = {
    "":               "section.html",        # all archive (ë¦¬ìŠ¤íŠ¸)
    "works":          "section/works.html",  # ì¸ë„¤ì¼ ê·¸ë¦¬ë“œ
    "shop":           "section/shop.html",   # ì¸ë„¤ì¼ ê·¸ë¦¬ë“œ
    "glossary":       "section.html",
    "method":         "section.html",
    "thought":        "section.html",
    # í•˜ìœ„ ì„¹ì…˜ë“¤ì€ transparent ê·œì¹™ìœ¼ë¡œ ì²˜ë¦¬
}

# íŒŒì¼ ê²½ë¡œ(ë¯¸ë””ì–´)ë¡œ ê°„ì£¼í•  ì»¤ìŠ¤í…€ í‚¤
FILE_URL_KEYS = {"thumbnail", "image", "poster"}

# ---------- ì •ê·œì‹ ----------
MD_RE = re.compile(r"\.md$", re.IGNORECASE)

# â˜… title(ì˜µì…˜)ê¹Œì§€ ìº¡ì²˜í•˜ë„ë¡ ìˆ˜ì •
# â˜… ì´ë¯¸ì§€/ë§í¬ ë§ˆí¬ë‹¤ìš´ì„ ë” ëŠìŠ¨í•˜ê²Œ ì¡ë„ë¡ ìˆ˜ì •
IMG_LINK_RE = re.compile(
    r'!\[([^\]]*)\]\('          # ![alt](
    r'\s*([^)"]+?)\s*'          # ê²½ë¡œ: " ë‚˜ ) ë‚˜ì˜¤ê¸° ì „ê¹Œì§€
    r'(?:\"([^\"]*)\")?'        # "title" (ì˜µì…˜)
    r'\)'
)

LINK_RE = re.compile(
    r'\[([^\]]*)\]\('
    r'\s*([^)]+?)\s*'           # ê²½ë¡œ ì „ì²´ë¥¼ ë°›ì•„ì˜´
    r'\)'
)


DISABLED_LINK_RE = re.compile(r'\[([^\]]+)\]\(\s*#disabled\s*\)')

FM_TOML_RE = re.compile(r'^\s*\+{3}\s*\n(.*?)\n\+{3}\s*', re.S)
FM_YAML_RE = re.compile(r'^\s*-{3}\s*\n(.*?)\n-{3}\s*', re.S)

DATE_FLEX_RE = re.compile(r'^\s*(?P<y>\d{4})(?:[.\-\/\s]*(?P<m>\d{1,2})(?:[.\-\/\s]*(?P<d>\d{1,2}))?)?\s*$')

# Obsidian [[path|label]]
WIKILINK_GLOBAL_RE = re.compile(r'\[\[\s*([^\]|]+?)(?:\|([^\]]+))?\s*\]\]')
WIKILINK_ONE_RE    = re.compile(r'^\[\[\s*([^\]|]+?)(?:\|([^\]]+))?\s*\]\]$')

KEYVAL_LINE_TOML = re.compile(r'^\s*([A-Za-z0-9_\-]+)\s*=\s*(.+?)\s*$')
KEYVAL_LINE_YAML = re.compile(r'^\s*([A-Za-z0-9_\-]+)\s*:\s*(.+?)\s*$')

# ìƒìœ„ì— ìœ ì§€í•  í”„ë¡ íŠ¸ë§¤í„° í‚¤
KEEP_TOPLEVEL = {"title", "date", "template", "draft", "weight", "slug", "taxonomies"}

# ìˆ«ìë¡œ ë‹¤ë£¨ê³  ì‹¶ì€ ì»¤ìŠ¤í…€ í‚¤ (ë”°ì˜´í‘œ ì—†ì´)
NUMERIC_KEYS = {"doc_no", "date_year"}

# ---------- ìœ í‹¸ ----------
def read_file(p):
    with open(p, "r", encoding="utf-8") as f:
        return f.read()

def write_file(p, s):
    os.makedirs(os.path.dirname(p), exist_ok=True)
    with open(p, "w", encoding="utf-8") as f:
        f.write(s)

def is_subsection(section_rel: str) -> bool:
    # ì˜ˆ: 'works/project' ê°™ì€ í•˜ìœ„ ì„¹ì…˜ë§Œ True
    return "/" in section_rel if section_rel else False

# ---------- ë¯¸ë””ì–´ ê²½ë¡œ ì¬ì‘ì„± ----------
def to_web_media_path(doc_parent_rel: str, media_rel: str) -> str:
    """
    'media/...' (ìƒëŒ€), '/media/...' (ì ˆëŒ€), './media/...', '../media/...' ë“±ì„
    ì‚¬ì´íŠ¸ ì ˆëŒ€ ê²½ë¡œë¡œ ì •ê·œí™”.
    ê·¸ ì™¸(http, data URL ë“±)ëŠ” ê·¸ëŒ€ë¡œ ë°˜í™˜.
    """
    s = (media_rel or "").strip()
    if not s:
        return s

    # 0) ì™¸ë¶€ URLì€ ì†ëŒ€ì§€ ì•ŠìŒ
    if s.startswith("http://") or s.startswith("https://") or s.startswith("data:"):
        return s

    # 1) ì´ë¯¸ ì‚¬ì´íŠ¸ ì ˆëŒ€ ê²½ë¡œ '/media/...'ë©´ ê±°ì˜ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    if s.startswith("/media/"):
        out = s
        if doc_parent_rel:
            # '/media/<parent>/media/' ê°™ì€ ì¤‘ë³µ íŒ¨í„´ ë°©ì§€
            out = out.replace(f"/media/{doc_parent_rel}/media/", f"/media/{doc_parent_rel}/")
        return out

    # 2) ê·¸ ì™¸ ìƒëŒ€ ê²½ë¡œì—ì„œ 'media/' ë¶€ë¶„ì„ ì°¾ì•„ ê·¸ ë’¤ë§Œ ì‚¬ìš©
    #    ì˜ˆ: './media/pr-001/...' â†’ 'media/pr-001/...'
    #        '../x/media/pr-001/...' â†’ 'media/pr-001/...'
    idx = s.find("media/")
    if idx != -1:
        s = s[idx:]  # 'media/...'

    # 3) Vault ê¸°ì¤€ ìƒëŒ€ê²½ë¡œ 'media/...'
    if s.startswith("media/"):
        rel = s[len("media/"):]  # e.g. 'pr-001/foo.webp'
        base = f"/media/{doc_parent_rel}/" if doc_parent_rel else "/media/"
        path = os.path.join(base, rel).replace("\\", "/")
        return path.replace("\\", "/").replace("//", "/")

    # 4) ê·¸ ì™¸ëŠ” ê±´ë“œë¦¬ì§€ ì•ŠìŒ
    return media_rel



def rewrite_media_paths(doc_rel_dir: str, text: str) -> str:
    # â˜… ì´ë¯¸ì§€: titleê¹Œì§€ ë³´ì¡´í•˜ë©´ì„œ srcë§Œ ì ˆëŒ€ê²½ë¡œë¡œ
    def repl_img(m):
        alt, mrel, title = (m.group(1) or ""), (m.group(2) or ""), (m.group(3) or "")
        abs_src = to_web_media_path(doc_rel_dir, mrel)
        if title:
            return f'![{alt}]({abs_src} "{title}")'
        return f'![{alt}]({abs_src})'
    text = IMG_LINK_RE.sub(repl_img, text)

    # ì¼ë°˜ ë§í¬ëŠ” ì¢…ì „ëŒ€ë¡œ
    def repl_link(m):
        label, mrel = m.group(1), m.group(2)
        return f'[{label}]({to_web_media_path(doc_rel_dir, mrel)})'
    text = LINK_RE.sub(repl_link, text)
    return text

def copy_media_folder(src_doc_dir: str, doc_parent_rel: str):
    media_src = os.path.join(src_doc_dir, "media")
    if not os.path.isdir(media_src):
        return

    dest_media_dir = os.path.join(DEST, "static", "media", doc_parent_rel)

    # ğŸ”¥ ì¶”ê°€: ê¸°ì¡´ media í´ë” ì „ì²´ ì‚­ì œ í›„ ì¬ë³µì‚¬ (ì”ì—¬íŒŒì¼ ì œê±°)
    if os.path.isdir(dest_media_dir):
        shutil.rmtree(dest_media_dir)

    os.makedirs(dest_media_dir, exist_ok=True)

    for root, dirs, files in os.walk(media_src):
        rel = os.path.relpath(root, media_src)
        for d in dirs:
            os.makedirs(os.path.join(dest_media_dir, rel, d), exist_ok=True)
        for f in files:
            sp = os.path.join(root, f)
            dp = os.path.join(dest_media_dir, rel, f)
            os.makedirs(os.path.dirname(dp), exist_ok=True)
            shutil.copy2(sp, dp)
    for root, dirs, files in os.walk(media_src):
        rel = os.path.relpath(root, media_src)
        for d in dirs:
            os.makedirs(os.path.join(dest_media_dir, rel, d), exist_ok=True)
        for f in files:
            sp = os.path.join(root, f)
            dp = os.path.join(dest_media_dir, rel, f)
            os.makedirs(os.path.dirname(dp), exist_ok=True)
            shutil.copy2(sp, dp)

# ---------- í”„ë¡ íŠ¸ë§¤í„° ----------
def split_front_matter(text: str):
    m = FM_TOML_RE.match(text)
    if m:
        return ("toml", m.group(1), text[m.end():])
    m = FM_YAML_RE.match(text)
    if m:
        return ("yaml", m.group(1), text[m.end():])
    return (None, "", text)

def assemble_front_matter(kind: str, head: str, body: str) -> str:
    if kind == "toml":
        return "+++\n" + head.strip("\n") + "\n+++\n" + body
    if kind == "yaml":
        return "---\n" + head.strip("\n") + "\n---\n" + body
    return body

# ---------- ë‚ ì§œ ì •ê·œí™” ----------
def _norm_date_any(s: str) -> str | None:
    m = DATE_FLEX_RE.search(str(s))
    if not m:
        return None
    y = int(m.group('y')); mth = m.group('m'); day = m.group('d')
    if mth is None:
        mm, dd = 1, 1
    else:
        mm = int(mth)
        if not (1 <= mm <= 12): return None
        if day is None:
            dd = 1
        else:
            dd = int(day)
            if not (1 <= dd <= 31): return None
    return f"{y:04d}-{mm:02d}-{dd:02d}"

def sanitize_front_matter_text(text: str) -> tuple[str, bool]:
    changed = False
    kind, head, body = split_front_matter(text)
    if not kind:
        return text, False

    if kind == "toml":
        m_date = re.search(r'^\s*date\s*=\s*(.+?)\s*$', head, re.M)
        if m_date:
            n = _norm_date_any(m_date.group(1))
            head2 = re.sub(r'^\s*date\s*=\s*.+?$', f'date = {n}' if n else '', head, flags=re.M)
            if head2 != head: head = head2; changed = True
        else:
            m_sort = re.search(r'^\s*date_sort\s*=\s*(.+?)\s*$', head, re.M)
            if m_sort:
                n = _norm_date_any(m_sort.group(1))
                if n: head = head.rstrip("\n") + f"\ndate = {n}\n"; changed = True

    elif kind == "yaml":
        head2 = re.sub(r'^\s*date\s*=\s*(.+?)\s*$', r'date: \1', head, flags=re.M)
        if head2 != head: head = head2; changed = True
        m_date = re.search(r'^\s*date\s*:\s*(.+?)\s*$', head, re.M)
        if m_date:
            n = _norm_date_any(m_date.group(1))
            head2 = re.sub(r'^\s*date\s*:\s*.+?$', f'date: {n}' if n else '', head, flags=re.M)
            if head2 != head: head = head2; changed = True
        else:
            m_sort = re.search(r'^\s*date_sort\s*:\s*(.+?)\s*$', head, re.M)
            if m_sort:
                n = _norm_date_any(m_sort.group(1))
                if n: head = head.rstrip("\n") + f"\ndate: {n}\n"; changed = True

    return assemble_front_matter(kind, head, body), changed

def pre_sanitize_content():
    content_root = pathlib.Path(DEST) / "content"
    if not content_root.exists():
        return
    fixed = 0
    for p in content_root.rglob("*.md"):
        t = read_file(p)
        new_t, changed = sanitize_front_matter_text(t)
        if changed:
            write_file(str(p), new_t)
            fixed += 1
            print(f"FIXED {p}")
    if fixed:
        print(f"[sanitize] fixed {fixed} file(s).")

def ensure_normalized_date(text: str) -> str:
    new_t, _ = sanitize_front_matter_text(text)
    return new_t

# ---------- ìœ„í‚¤ë§í¬/ê²½ë¡œ ----------
def _is_vault_abs(path: str) -> bool:
    if not path: return False
    head = path.split("/", 1)[0].lower()
    return head in ROOT_SECTIONS

def _split_lang_and_ext(last: str):
    lang = None
    name = last
    if name.endswith(".md"): name = name[:-3]
    if name.endswith(".kr"):
        name = name[:-3]
        lang = "kr"
    return name, lang

def _slugify_segment(name: str) -> str:
    return name.strip().replace(" ", "-").lower()

def _vault_path_to_site_href(raw_path: str, doc_parent_rel: str) -> str:
    raw_path = (raw_path or "").strip()
    if not raw_path:
        return "/"
    if raw_path.startswith("/"):
        vault_rel = raw_path.lstrip("/")
    elif _is_vault_abs(raw_path):
        vault_rel = raw_path
    else:
        p = pathlib.PurePosixPath(doc_parent_rel) / raw_path
        parts = []
        for part in str(p).split('/'):
            if part in ("", "."): continue
            if part == "..":
                if parts: parts.pop()
                continue
            parts.append(part)
        vault_rel = "/".join(parts)

    parts = [seg for seg in vault_rel.split("/") if seg]
    if not parts: return "/"
    base, lang = _split_lang_and_ext(parts[-1])
    parts[-1] = _slugify_segment(base)
    prefix = "/kr" if lang == "kr" else ""
    href = prefix + "/" + "/".join(parts) + "/"
    href = href.replace("//", "/")
    return href

def rewrite_wikilinks_in_body(kind: str, head: str, body: str, doc_parent_rel: str):
    def repl(m):
        path = (m.group(1) or "").strip()
        label = (m.group(2) or "").strip()
        href = _vault_path_to_site_href(path, doc_parent_rel)
        if not label:
            label = href.strip("/").split("/")[-1] or href
        return f'[{label}]({href})'
    return head, WIKILINK_GLOBAL_RE.sub(repl, body)

# ---------- ì»¤ìŠ¤í…€ ë©”íƒ€ â†’ extra ì´ë™ ----------
def _strip_quotes(s: str) -> str:
    s = s.strip()
    if (s.startswith('"') and s.endswith('"')) or (s.startswith("'") and s.endswith("'")):
        return s[1:-1]
    return s

def move_custom_fields_into_extra(text: str, doc_parent_rel: str) -> str:
    kind, head, body = split_front_matter(text)
    if not kind:
        return text

    def handle_link_value(raw: str):
        raw0 = _strip_quotes(raw)
        m_w = WIKILINK_ONE_RE.match(raw0)
        if m_w:
            path = (m_w.group(1) or "").strip()
            label = (m_w.group(2) or "").strip()
        else:
            path = raw0
            label = ""
        href = _vault_path_to_site_href(path, doc_parent_rel)
        if not label:
            label = href.strip("/").split("/")[-1] or href
        return href, label

    def label_from_wikilink_or_text(raw: str) -> str:
        raw0 = _strip_quotes(raw)
        m_w = WIKILINK_ONE_RE.match(raw0)
        if m_w and m_w.group(2):
            return m_w.group(2).strip()
        return raw0

    def media_href_from_value(raw: str) -> str:
        """
        FILE_URL_KEYSìš©:
          - ê°’ ì–´ë”˜ê°€ì— [[...]] ê°€ ìˆìœ¼ë©´ ë‚´ë¶€ ê²½ë¡œë§Œ ì¶”ì¶œ
          - '/media/...'ë¡œ ì‹œì‘í•˜ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©(ì¤‘ë³µ '/media/<parent>/media/'ëŠ” ì •ê·œí™”)
          - '...media/...'ë¥¼ í¬í•¨í•˜ë©´ ê·¸ ì§€ì ë¶€í„°ë¥¼ 'media/...'ë¡œ ê°„ì£¼í•´ ì›¹ ê²½ë¡œë¡œ ë³€í™˜
        """
        raw0 = _strip_quotes(raw).strip()
        if not raw0:
            return raw0

        # 1) ìœ„í‚¤ë§í¬ê°€ ë¬¸ìì—´ ì–´ë”˜ê°€ì— ì„ì—¬ ìˆëŠ” ê²½ìš°ê¹Œì§€ ì²˜ë¦¬
        m_any = re.search(r'\[\[\s*([^\]|]+)', raw0)
        candidate = (m_any.group(1).strip() if m_any else raw0)

        # 2) ì´ë¯¸ ì‚¬ì´íŠ¸ ì ˆëŒ€ê²½ë¡œì¸ ê²½ìš°
        if candidate.startswith("/media/"):
            out = candidate
            if doc_parent_rel:
                out = out.replace(f"/media/{doc_parent_rel}/media/", f"/media/{doc_parent_rel}/")
            return out

        # 3) '...media/...' í¬í•¨ â†’ ê·¸ ì§€ì ë¶€í„° 'media/...'ë§Œ ì¶”ì¶œ
        idx = candidate.find("media/")
        if idx != -1:
            media_rel = candidate[idx:]  # 'media/...'
            return to_web_media_path(doc_parent_rel, media_rel)

        # 4) ê·¸ ì™¸ëŠ” ì›ë³¸ ìœ ì§€(ì™¸ë¶€ URL ë“±)
        return candidate

    if kind == "toml":
        lines = head.splitlines()
        keep_lines, moved = [], {}
        for ln in lines:
            m2 = KEYVAL_LINE_TOML.match(ln)
            if not m2:
                keep_lines.append(ln)
                continue

            k, v = m2.group(1), m2.group(2)
            k_low = k.lower()

            # ìƒë‹¨ ìœ ì§€í•´ì•¼ í•˜ëŠ” í‚¤ë“¤ì€ ê·¸ëŒ€ë¡œ ë‚¨ê¹€
            if k_low in KEEP_TOPLEVEL:
                keep_lines.append(ln)
                continue

            # ğŸ”¢ ìˆ«ì í•„ë“œ(doc_no, date_year ë“±)ëŠ” ë”°ì˜´í‘œ ì—†ì´ ìˆ«ìë¡œ ì €ì¥
            if k_low in NUMERIC_KEYS:
                raw0 = _strip_quotes(v).strip()
                if raw0 == "":
                    val = "0"
                else:
                    try:
                        val = str(int(raw0))
                    except ValueError:
                        mnum = re.search(r"\d+", raw0)
                        val = mnum.group(0) if mnum else "0"
                moved[k] = val
                continue

            # ë§í¬ í•„ë“œ
            if k_low == "link":
                href, label = handle_link_value(v)
                moved["link"] = f'"{href}"'
                moved["link_label"] = f'"{label}"'

            # ì¸ë„¤ì¼/ì´ë¯¸ì§€/í¬ìŠ¤í„° ë“± íŒŒì¼ ê²½ë¡œ í•„ë“œ
            elif k_low in FILE_URL_KEYS:
                href = media_href_from_value(v)
                moved[k] = f'"{href}"'

            # ê·¸ ì™¸ ì»¤ìŠ¤í…€ í•„ë“œëŠ” ë¬¸ìì—´ë¡œ extraì— ë„£ê¸°
            else:
                label = label_from_wikilink_or_text(v)
                if not (label.startswith('"') and label.endswith('"')):
                    label = f'"{label}"'
                moved[k] = label


        head2 = "\n".join([l for l in keep_lines if l.strip() != ""])
        if re.search(r'^\s*\[extra\]\s*$', head2, re.M):
            head2 = re.sub(
                r'(\[extra\][^\[]*)',
                lambda mm: mm.group(1) + "".join([f'\n{k} = {moved[k]}' for k in moved]),
                head2, count=1, flags=re.S
            )
        elif moved:
            extra_lines = ["", "[extra]"] + [f'{k} = {moved[k]}' for k in moved]
            head2 = (head2 + "\n" + "\n".join(extra_lines)).strip("\n")
        return assemble_front_matter(kind, head2, body)

    if kind == "yaml":
        lines = head.splitlines()
        keep_lines, moved = [], {}
        for ln in lines:
            m2 = KEYVAL_LINE_YAML.match(ln)
            if not m2:
                keep_lines.append(ln)
                continue

            k, v = m2.group(1), m2.group(2)
            k_low = k.lower()

            # ìƒë‹¨ ìœ ì§€í•´ì•¼ í•˜ëŠ” í‚¤ë“¤ì€ ê·¸ëŒ€ë¡œ ë‘ê¸°
            if k_low in KEEP_TOPLEVEL:
                keep_lines.append(ln)
                continue

            # ğŸ”¢ ìˆ«ì í•„ë“œ(doc_no, date_year ë“±)ëŠ” ë”°ì˜´í‘œ ì—†ì´ ìˆ«ìë¡œ ì €ì¥
            if k_low in NUMERIC_KEYS:
                raw0 = _strip_quotes(v).strip()
                if raw0 == "":
                    val = "0"
                else:
                    try:
                        val = str(int(raw0))
                    except ValueError:
                        mnum = re.search(r"\d+", raw0)
                        val = mnum.group(0) if mnum else "0"
                moved[k] = val
                continue

            # link í•„ë“œ: href/label ë¶„ë¦¬
            if k_low == "link":
                href, label = handle_link_value(v)
                moved["link"] = f'"{href}"'
                moved["link_label"] = f'"{label}"'

            # íŒŒì¼ ê²½ë¡œ í•„ë“œ(thumbnail, image, poster)
            elif k_low in FILE_URL_KEYS:
                href = media_href_from_value(v)
                moved[k] = f'"{href}"'

            # ê·¸ ì™¸ ì»¤ìŠ¤í…€ í•„ë“œ â†’ ë¬¸ìì—´ë¡œ extraì— ë„£ê¸°
            else:
                label = label_from_wikilink_or_text(v)
                if not (label.startswith('"') and label.endswith('"')):
                    label = f'"{label}"'
                moved[k] = label


        head2 = "\n".join([l for l in keep_lines if l.strip() != ""])
        if re.search(r'^\s*extra\s*:\s*$', head2, re.M):
            head2 = re.sub(
                r'(^\s*extra\s*:\s*$)',
                lambda mm: mm.group(1) + "".join([f'\n  {k}: {moved[k]}' for k in moved]),
                head2, count=1, flags=re.M
            )
        elif moved:
            extra_lines = ["extra:"] + [f'  {k}: {moved[k]}' for k in moved]
            head2 = (head2.rstrip("\n") + "\n" + "\n".join(extra_lines)).strip("\n")
        return assemble_front_matter(kind, head2, body)

    return text

# ---------- ì´ë¯¸ì§€ title ì˜µì…˜ íŒŒì„œ & ë³€í™˜ (NEW) ----------
def _parse_img_title_directives(title: str) -> dict:
    """
    title ì˜ˆì‹œ: 'fixed:480; bordered; caption=Hello world'
    ë°˜í™˜: {'fixed': '480', 'bordered': True, 'caption': 'Hello world'}
    """
    out = {}
    if not title:
        return out

    tokens = [t.strip() for t in title.split(';') if t.strip()]
    for raw in tokens:
        # 1) 'caption=...' ë“± '=' ìš°ì„ 
        if '=' in raw:
            k, v = raw.split('=', 1)
            out[k.strip().lower()] = v.strip()
            continue
        # 2) 'fixed:480' / 'grid:3' ë“± ':'
        if ':' in raw:
            k, v = raw.split(':', 1)
            out[k.strip().lower()] = v.strip()
            continue
        # 3) ë‹¨ë… í”Œë˜ê·¸
        out[raw.lower()] = True

    return out

def transform_markdown_images_with_directives(doc_rel_dir: str, text: str) -> str:
    """
    media ê²½ë¡œ ì¬ì‘ì„± ì´í›„ ì‹¤í–‰.
    - ![alt](/media/.. "ì˜µì…˜") â†’ <figure class="..."><img ...><figcaption>...</figcaption></figure>
    - ì˜µì…˜ì´ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë‘ .
    - gridw:### â†’ figureì— data-gridw="###" ë¶€ì—¬ (wrap ë‹¨ê³„ì—ì„œ ì‚¬ìš©)
    """
    def repl(m):
        alt   = (m.group(1) or "").strip()
        src   = (m.group(2) or "").strip()
        title = (m.group(3) or "").strip()

        # ì•ˆì „ ì ˆëŒ€ê²½ë¡œí™” (ì¬ë³´ì •)
        src_abs = to_web_media_path(doc_rel_dir, src)

        opts = _parse_img_title_directives(title)
        if not opts:
            return f'![{alt}]({src_abs})'

        classes, style = [], ""

        # âœ… ì—¬ê¸°ë¥¼ ë°˜ë“œì‹œ repl() ì•ˆìœ¼ë¡œ ë“¤ì—¬ì“°ê¸°
        # ìº¡ì…˜ í‚¤ê°€ ìˆì„ ë•Œë§Œ ì¶œë ¥ (ë¹ˆ ê°’ì´ë©´ ì¶œë ¥ ì•ˆ í•¨)
        has_caption = ('caption' in opts)
        caption_text = (opts.get('caption') or "").strip() if has_caption else ""

        if 'full' in opts: 
            classes.append('img--full')
        if 'bordered' in opts: 
            classes.append('img--bordered')

        fixed = opts.get('fixed') or opts.get('max')
        if fixed:
            classes.append('img--fixed')
            try:
                w = int(str(fixed).strip().replace('px',''))
                style += f'max-width:{w}px;'
            except:
                pass

        grid = opts.get('grid')
        if grid:
            classes.append(f'grid-{grid}')

        # gridw ì§€ì›
        data_attr = ""
        gridw = opts.get('gridw')
        if gridw:
            gridw_clean = str(gridw).strip().replace('px','')
            if gridw_clean.isdigit():
                data_attr = f' data-gridw="{gridw_clean}"'

        cls_attr   = f' class="{" ".join(classes)}"' if classes else ''
        style_attr = f' style="{style}"' if style else ''

        fig = []
        fig.append(f'<figure{cls_attr}{style_attr}{data_attr}>')
        fig.append(f'  <img src="{src_abs}" alt="{alt}">')
        if has_caption and caption_text:
            fig.append(f'  <figcaption>{caption_text}</figcaption>')
        fig.append(f'</figure>')
        return "\n".join(fig)

    return IMG_LINK_RE.sub(repl, text)



def transform_disabled_links(text: str) -> str:
    """
    [Label](#disabled) â†’ <a class="is-disabled" aria-disabled="true">Label</a>
    (front matterì˜ link ê°’ê³¼ëŠ” ë³„ê°œ, ë³¸ë¬¸ ë§ˆí¬ë‹¤ìš´ ì•µì»¤ë§Œ ëŒ€ìƒ)
    """
    return DISABLED_LINK_RE.sub(
        r'<a class="is-disabled" aria-disabled="true">\1</a>',
        text
    )


# ---------- grid:N figure ì—°ì† ë¬¶ê¸° (NEW) ----------
def wrap_grid_runs(text: str) -> str:
    """
    ì—°ì†ëœ <figure ... class="... grid-N ..."> ë¸”ë¡ë“¤ì„
    <div class="img-grid cols-N"> ... </div>ìœ¼ë¡œ ê°ì‹¼ë‹¤.
    - Nì´ ê°™ì€ ê²ƒë“¤ë§Œ ê°™ì€ ê·¸ë£¹
    - ì‚¬ì´ì— ê³µë°±/ê°œí–‰ë§Œ ìˆëŠ” ê²½ìš° ì—°ì†ìœ¼ë¡œ ê°„ì£¼
    - â˜… ì²« figureì˜ data-gridwê°€ ìˆìœ¼ë©´ ì»¨í…Œì´ë„ˆì— max-width ì ìš©
    """
    fig_re = re.compile(
        r'(<figure(?P<attrs>[^>]*?)class="(?P<class>[^"]*\bgrid-(?P<n>\d+)\b[^"]*)"(?P<tail>[^>]*)>.*?</figure>)',
        re.S
    )

    out, i = [], 0
    pending = []   # [(start, end, n, html, attrs+tail)]
    pending_n = None

    def _flush_group():
        if not pending:
            return ""
        first_attrs = pending[0][4]
        # data-gridw ì¶”ì¶œ
        m_w = re.search(r'data-gridw="(\d+)"', first_attrs or "")
        mw = m_w.group(1) if m_w else None
        style_attr = f' style="max-width:{mw}px;margin-left:auto;margin-right:auto;"' if mw else ""
        # ê·¸ëŒ€ë¡œ ë¬¶ê¸° (ê¸°ì¡´ ìº¡ì…˜ ìœ ì§€)
        html_parts = [f'<div class="img-grid cols-{pending_n}"{style_attr}>']
        html_parts += [h for _,__,___,h,____ in pending]
        html_parts.append('</div>')
        return "".join(html_parts)

    for m in fig_re.finditer(text):
        start, end = m.span()
        n = m.group('n')
        html = m.group(0)
        attrs_tail = (m.group('attrs') or '') + (m.group('tail') or '')

        gap = text[i:start]
        only_ws = (gap.strip() == "")

        if not pending:
            out.append(gap)  # ì•ì˜ ì¼ë°˜ í…ìŠ¤íŠ¸ flush
            pending = [(start, end, n, html, attrs_tail)]
            pending_n = n
        else:
            if only_ws and n == pending_n:
                pending.append((start, end, n, html, attrs_tail))
            else:
                # ì´ì „ ê·¸ë£¹ ë§ˆê°
                out.append(_flush_group())
                # ë‹¤ë¥¸ ì½˜í…ì¸ /ë‹¤ë¥¸ N ì²˜ë¦¬
                out.append(gap)
                pending = [(start, end, n, html, attrs_tail)]
                pending_n = n

        i = end

    # ë§ˆì§€ë§‰ ì”ì—¬ ì²˜ë¦¬
    tail = text[i:]
    if pending:
        out.append(_flush_group())
        out.append(tail)
    else:
        out.append(tail)

    return "".join(out)



# ---------- ì„¹ì…˜ ì¸ë±ìŠ¤ ìƒì„±/ë³´ì • ----------
def ensure_index_for_section(section_rel: str):
    dest_dir = os.path.join(DEST, "content", section_rel) if section_rel else os.path.join(DEST, "content")
    os.makedirs(dest_dir, exist_ok=True)
    en_path = os.path.join(dest_dir, "_index.md")
    kr_path = os.path.join(dest_dir, "_index.kr.md")

    # â”€â”€â”€ (1) about/contactëŠ” 'ë¦¬ë””ë ‰íŠ¸ ì „ìš©'ìœ¼ë¡œ ê°•ì œ ìƒì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    REDIRECT_SECTIONS = {
        "about":   {"en": "/about/about/",   "kr": "/kr/about/about/"},
        "contact": {"en": "/contact/contact/","kr": "/kr/contact/contact/"},
    }
    if section_rel in REDIRECT_SECTIONS:
        en_redirect = REDIRECT_SECTIONS[section_rel]["en"]
        kr_redirect = REDIRECT_SECTIONS[section_rel]["kr"]

        en_text = f"""+++
title = "{TITLES_EN.get(section_rel, section_rel)}"
redirect_to = "{en_redirect}"
+++ 
"""
        kr_text = f"""+++
title = "{TITLES_KR.get(section_rel, section_rel)}"
redirect_to = "{kr_redirect}"
+++ 
"""

        write_file(en_path, en_text)
        write_file(kr_path, kr_text)
        print(f"CREATE (redirect) _index.* @ {section_rel or '/'}")
        return

    # â”€â”€â”€ (2) ê·¸ ì™¸ ì„¹ì…˜ì€ ê¸°ì¡´ ë¡œì§(í…œí”Œë¦¿/transparent) ìœ ì§€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    needs_transparent = is_subsection(section_rel)
    tmpl = SECTION_TEMPLATES.get(section_rel, "section.html")

    if not os.path.exists(en_path):
        title = TITLES_EN.get(section_rel, "archive")
        en_lines = ["+++", f'title = "{title}"', f'template = "{tmpl}"']
        if needs_transparent: en_lines.append("transparent = true")
        en_lines.append('# sort_by = "extra.date_sort"')
        en_lines.append("+++")
        write_file(en_path, "\n".join(en_lines) + "\n")
        print(f"CREATE _index.md @ {section_rel or '/'}")

    if not os.path.exists(kr_path):
        title = TITLES_KR.get(section_rel, "ì•„ì¹´ì´ë¸Œ")
        kr_lines = ["+++", f'title = "{title}"', f'template = "{tmpl}"']
        if needs_transparent: kr_lines.append("transparent = true")
        kr_lines.append('# sort_by = "extra.date_sort"')
        kr_lines.append("+++")
        write_file(kr_path, "\n".join(kr_lines) + "\n")
        print(f"CREATE _index.kr.md @ {section_rel or '/'}")

    def ensure_transparent_flag(path: str):
        if not needs_transparent or not os.path.exists(path): return
        txt = read_file(path)
        if "transparent" in txt: return
        kind, head, body = split_front_matter(txt)
        new_txt = assemble_front_matter(kind, "transparent = true\n" + head.lstrip("\n"), body) if kind else "transparent = true\n" + txt
        write_file(path, new_txt); print(f"PATCH transparent=true â†’ {path}")

    ensure_transparent_flag(en_path); ensure_transparent_flag(kr_path)

# ---------- ë³µì‚¬ ì œì™¸ ê·œì¹™ ----------
def should_skip_as_section_index(rel_path_from_vault: str) -> bool:
    lower = rel_path_from_vault.lower()
    return (
        lower.endswith("about/index.md") or lower.endswith("about/index.kr.md") or
        lower.endswith("contact/index.md") or lower.endswith("contact/index.kr.md")
    )

# ---------- ëª©ì ì§€ ì •ë¦¬ ----------
def clean_destination():
    content_dir = os.path.join(DEST, "content")
    media_dir   = os.path.join(DEST, "static", "media")
    if os.path.isdir(content_dir):
        shutil.rmtree(content_dir); print("CLEAN content/ (removed)")
    os.makedirs(content_dir, exist_ok=True)
    if os.path.isdir(media_dir):
        shutil.rmtree(media_dir); print("CLEAN static/media/ (removed)")

# ---------- ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ----------
def process_markdown(src_path: str, rel_path_from_vault: str):
    if should_skip_as_section_index(rel_path_from_vault):
        print(f"SKIP {rel_path_from_vault} (handled by auto _index)")
        return

    text = read_file(src_path)
    doc_parent_rel = str(pathlib.PurePosixPath(rel_path_from_vault).parent)
    if doc_parent_rel == ".": doc_parent_rel = ""

    # í”„ë¡ íŠ¸ë§¤í„°/ë³¸ë¬¸ ë¶„ë¦¬
    kind, head, body = split_front_matter(text)

    # 1) media ê²½ë¡œ + ë³¸ë¬¸ ìœ„í‚¤ë§í¬ ë³€í™˜
    body = rewrite_media_paths(doc_parent_rel, body)
    head, body = rewrite_wikilinks_in_body(kind, head, body, doc_parent_rel)

    # â˜… 1-2) ì´ë¯¸ì§€ title ì˜µì…˜ â†’ figure ë³€í™˜
    body = transform_markdown_images_with_directives(doc_parent_rel, body)

    # â˜… 1-2.5) ì¤€ë¹„ì¤‘ ë§í¬ ë³€í™˜ ([...](#disabled) â†’ <a class="is-disabled"...>)
    body = transform_disabled_links(body)

    # â˜… 1-3) ì—°ì† grid:N figure ë¬¶ê¸° (NEW)
    body = wrap_grid_runs(body)

    # í•©ì¹˜ê¸°
    text2 = assemble_front_matter(kind, head, body)

    # 2) ë‚ ì§œ ì •ê·œí™”
    text2 = ensure_normalized_date(text2)

    # 3) ì»¤ìŠ¤í…€ ë©”íƒ€ extraë¡œ ì´ë™ (linkâ†’href/label, íŒŒì¼í‚¤â†’ì›¹ê²½ë¡œ, ê·¸ ì™¸ëŠ” ë¼ë²¨)
    text2 = move_custom_fields_into_extra(text2, doc_parent_rel)

    # 4) ëª©ì ì§€ë¡œ ë³µì‚¬
    dest_rel = rel_path_from_vault
    dest_path = os.path.join(DEST, "content", dest_rel)
    os.makedirs(os.path.dirname(dest_path), exist_ok=True)
    write_file(dest_path, text2)

    # 5) media í´ë” ë³µì‚¬
    copy_media_folder(os.path.dirname(src_path), doc_parent_rel)

    print(f"OK  {rel_path_from_vault} â†’ content/{dest_rel}")

def main():
    # 0) ê¹¨ë—í•œ ë™ê¸°í™”
    clean_destination()

    # 1) í•„ìˆ˜ ì„¹ì…˜ ì¸ë±ìŠ¤ ë³´ì¥ (+ í…œí”Œë¦¿/transparent ê·œì¹™ ì ìš©)
    for sec in SECTIONS:
        ensure_index_for_section(sec)

    # 2) ì„¹ì…˜/í˜ì´ì§€ ë³µì‚¬
    for root in SRC_CONTENT_ROOTS:
        src_root = os.path.join(VAULT, root)
        if not os.path.isdir(src_root): continue
        for dirpath, _, filenames in os.walk(src_root):
            for fn in filenames:
                if not MD_RE.search(fn): continue
                if fn.startswith("_index"): continue  # Vaultì—” _index ì•ˆ ë§Œë“¦
                src_path = os.path.join(dirpath, fn)
                rel = os.path.relpath(src_path, VAULT).replace("\\", "/")
                process_markdown(src_path, rel)

    print("\nDone. Now run: zola serve")

if __name__ == "__main__":
    main()
